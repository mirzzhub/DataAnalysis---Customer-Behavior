# -*- coding: utf-8 -*-
"""eda analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GUmJOCg0gZEuhefmy-TA8ZQcGS14mhtB
"""

import pandas as pd

df = pd.read_csv('Hackathon_Working_Data.csv')

#list the columns that are to be calculated and just use that array name
#else directly mention them
num_cols = ['BILL_AMT','QTY','VALUE','PRICE']

#to calculate mean, median , standard deviation
mean_val = df[num_cols].mean()

med_val = df[num_cols].median()

std_val = df[['BILL_AMT','QTY','VALUE','PRICE']].std()

#to calculate mode
mode_val = df[['BILL_ID','GRP','SGRP','SSGRP','CMP','BRD','MBRD']].mode()

#to calculate range
range_val = df[num_cols].max() - df[num_cols].min()

#to calculate variance
variance_val =df[num_cols].var()

#to calculate IQR (interquartile range)
Q1 = df[num_cols].quantile(0.25)
Q3 = df[num_cols].quantile(0.75)
iqr = Q3 - Q1

cor_mat = df[num_cols].corr()

# Display correlation matrix

print("MEAN")
print(mean_val)
print(" ")
print("MEDIAN")
print(med_val)
print(" ")
print("STANDARD DEVIATION")
print(std_val)
print(" ")
print("MODE")
print(mode_val)
print(" ")
print("RANGE")
print(range_val)
print(" ")
print("VARIANCE")
print(variance_val)
print(" ")
print("IQR")
print(iqr)
print(" ")
print("CORRELATION MATRIX:")
print(cor_mat)

grp_data = df.groupby('SGRP')
#first group the data column that is needed and then apply agg function for that grp data

grp_stat = grp_data.agg({
    'BILL_AMT': ['mean','median'],
    'QTY' : ['mean','median'],
    'VALUE': ['mean','median'],
    'PRICE': ['mean','median']
})

def mode_func(x):
  return x.value_counts().index[0]

grp_stat['BILL_AMT','mode'] = grp_data['BILL_AMT'].agg(mode_func)
grp_stat['QTY','mode'] = grp_data['QTY'].agg(mode_func)
grp_stat['VALUE','mode'] = grp_data['VALUE'].agg(mode_func)
grp_stat['PRICE','mode'] = grp_data['PRICE'].agg(mode_func)


print(grp_stat)

import matplotlib.pyplot as plt
import seaborn as sns


# Load your dataset into a DataFrame
df = pd.read_csv('Hackathon_Working_Data.csv')
df.columns = df.columns.str.lower()

# Create histograms for bill amount, quantity, and price
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.histplot(df['bill_amt'], bins=20, kde=True, color='skyblue')
plt.title('Histogram of Bill Amount')

plt.subplot(1, 3, 2)
sns.histplot(df['qty'], bins=20, kde=True, color='salmon')
plt.title('Histogram of Quantity')

plt.subplot(1, 3, 3)
sns.histplot(df['price'], bins=20, kde=True, color='green')
plt.title('Histogram of Price')

plt.tight_layout()
plt.show()

# Create box plots for bill amount, quantity, and price
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.boxplot(data=df, y='bill_amt', color='skyblue')
plt.title('Box Plot of Bill Amount')

plt.subplot(1, 3, 2)
sns.boxplot(data=df, y='qty', color='salmon')
plt.title('Box Plot of Quantity')

plt.subplot(1, 3, 3)
sns.boxplot(data=df, y='price', color='green')
plt.title('Box Plot of Price')

plt.tight_layout()
plt.show()

"""CLUSTERING :
1. Select relevant numerical columns for clustering
2. Standardize the data
3. Determine the optimal number of clusters using the elbow method
4. Plot the elbow curve
5. Based on the elbow curve, choose the optimal number of clusters
6. Apply K-means clustering
7. Add cluster labels to the original DataFrame
8. Display cluster centroids
9. Display the count of customers in each cluster

"""

#clustering

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# 1
X = df[['BILL_AMT', 'QTY', 'VALUE', 'PRICE']]

# 2
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# 4
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# 5
n_clusters = 3

# 6
kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# 7
df['Cluster'] = clusters

# 8
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
cluster_centers_df = pd.DataFrame(cluster_centers, columns=X.columns)
print("Cluster Centroids:")
print(cluster_centers_df)

# 9
cluster_counts = df['Cluster'].value_counts().sort_index()
print("\nCluster Counts:")
print(cluster_counts)

#using apriori algorithm for association analysis

import pandas as pd
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

#1
df_association = df[['BILL_ID', 'MBRD', 'BRD']]
#2
df_association = pd.get_dummies(df_association, columns=['MBRD', 'BRD'], prefix='', prefix_sep='')
#3
df_association = df_association.groupby('BILL_ID').sum()
#4
df_association = df_association.applymap(lambda x: 1 if x > 0 else 0)
#5
frequent_itemsets = apriori(df_association, min_support=0.01, use_colnames=True)
#6
association_rules_df = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# Display the association rules
print("Association Rules:")
print(association_rules_df)

"""APRIORI ALGORITHM :

1. Keeping relevant columns for association analysis
2. Convert product details into one-hot encoded format
3. Group by 'bill id' and sum up the one-hot encoded columns
4. Convert non-zero values to 1 for binary encoding
5. Perform Apriori algorithm to find frequent itemsets
6. Generating  association rules


DONE BY : MIRNALINI E R
"""